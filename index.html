<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Introducción al PLN, un enfoque clínico</title>

		<meta name="description" content="Unidad Técnica Snomed-CT España. Intro PLN Clinico">
		<meta name="author" content="Ignacio Martinez Soriano">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">

		<link rel="stylesheet" href="css/theme/beige.css" id="theme"> 
	
		<!-- <link rel="stylesheet" href="css/main.css"> 
			 <link rel="stylesheet" href="css/theme/league.css" id="theme"> -->
		
		
		<!-- For syntax highlighting  -->
        <link rel="stylesheet" href="lib/css/zenburn.css">
		
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
			
			<section data-background-transition="slide" data-background="images/Lorca01.jpg" style="background: rgba(0,129,195,.9); color: white" >
				<h4 style="color: rgba(255, 225, 255, .9)">
					"Introducción al Procesamiento Lenguaje Natural, un enfoque clínico."<br>
				</h4>
				<h3 style="color: rgba(255, 225, 255, .9)">
					Introducción al PLN Clínico. Evolución de la I.A.
				</h3>
				
				<p>Ignacio Martinez Soriano</p>
				
				<h4 style="color: rgba(255, 225, 255, .9)">
					15-Oct-2021 (Lorca) Webminar
					<img src="images/QR-PLNClinical.png" style="float: right; border: 0; padding: 0px; width: 20%; height: 20%"></img>

				</h4>

				<small><p>Unidad Técnica Snomed-CT<br>
						Hospital Universitario "Rafael Mendez"<br>
				</p></small>

			</section>
				<section>
					<h1>PLN-Clinical</h1>
					<ol>
						<li>Introduction.</li>
						<li>Procesamiento Lenguaje Natural.</li>
						<li>Word Embeddin.g</li>
						<li>Casos de Uso Enfoque Clínico.</li>						
						<li>Librerias y Frameworks</li>
						<li>Referencias en España de PLN</li>
						<li>Conclusiones y Propuestas</li>
					</ol>
				</section>
				<section>
					<section data-transition="zoom" data-background="images/cbms_fondo00.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">1. Introduction:</h2>
					</section>												

					<section>
						<h2>Introduction:</h2>
						<p><strong>¿De dónde partimos?</strong></p>
						<small><p>En Agosto de 1955 el profesor de Matemáticas, <strong>John McCarthy</strong>, organiza una Conferencia en la Universidad de Dartmouth, donde se define el concepto de Inteligencia Artificial,
							denotándola como  un proceso por el que <strong>"una máquina se puede comportar de formas que serían llamadas inteligentes si un ser humano hiciera eso"</strong></p>
						</small>
						<img src="images/McCarthy.jpg" style="border: 0; padding: 0px; width: 50%; height: 50%" style="float: right"> </img>

					</section>
					<section>
						<h3>¿Cómo se genera el Conocimiento?</h3>
						<p> Todo origen proviene de un <strong>"Dato"</strong>. (DataCentric)</p>
						<img src="images/Dato-Conocimiento.png" style="border: 0; padding: 0px; width: 55%; height: 55%"> </img>
						<p style="font-size: 15px">Fuente: matrixcpmsolutions.com<a href="https://matrixcpmsolutions.com/la-conciencia-del-dato/">Imagen</a></p>
					</section>
					<section>
						<h3>Sistema Tradicional VS Sistema M.L (IA)</h3>
						<p>¿Cómo aprende un algoritmo?</p>
						<img src="images/Reglas-ML.png" style="border: 0; padding: 0px; width: 75%; height: 75%"> </img>

					</section>
					<section>
						<h3>Machine Learning:</h3>
						<p> Disciplinas dentro de la I.A.</p>
						<img src="images/IAMLDeepLearning.png" style="border: 0; padding: 0px; width: 75%; height: 75%"> </img>

					</section>
					<section>
						<h3>¿Donde se encuentra el PLN?:</h3>
						<p>¿Qué tćnicas utiliza?</p>
						<img src="images/IA-ML-DL-PLN.png" style="border: 0; padding: 0px; width: 75%; height: 75%"> </img>
					</section>
					<section>
						<h3>Como aplicar ML al tipo de problemas</h3>
						<p>Cada problema tiene un algoritmo (1/3)</p>
						<img src="images/ML-approach.png" style="border: 0; padding: 0px; width: 75%; height: 75%"> </img>
					</section>
					<section>
						<h3>Como aplicar ML al tipo de problemas</h3>
						<p>Cada problema tiene un algoritmo (2/3)</p>
						<img src="images/ML-Sheet.png" style="border: 0; padding: 0px; width: 95%; height: 95%"> </img>
					</section>
					<section>
						<h3>Como aplicar ML al tipo de problemas</h3>
						<p>Cada problema tiene un algoritmo (3/3)</p>
						<img src="images/ML-Scikit_Learn.png" style="border: 0; padding: 0px; width: 95%; height: 95%"> </img>
					</section>
					<section>
						<h3>Ciclo de Vida del M.L.(1/2)</h3>
						<p>La clave está en el autoaprendizaje. o Ajuste de pesos.</p>
						<img src="images/ML-Ciclovida01.jpg" style="border: 0; padding: 0px; width: 90%; height: 90%"> </img>
						<p style="font-size: 15px">Fuente: AWS Amazon<a href="https://aws.amazon.com/es/blogs/aws-spanish/arquitectura-del-ciclo-de-vida-de-un-modelo-de-machine-learning-en-aws-una-demo-completa/">Imagen</a></p>
					</section>
					<section>
						<h3>Ciclo de Vida del M.L.(2/2)</h3>
						<p>La clave está en el autoaprendizaje. o Ajuste de pesos.</p>
						<img src="images/ML-Ciclovida02.jpg" style="border: 0; padding: 0px; width: 90%; height: 90%"> </img>
						<p style="font-size: 15px">Fuente: AWS Amazon<a href="https://aws.amazon.com/es/blogs/aws-spanish/arquitectura-del-ciclo-de-vida-de-un-modelo-de-machine-learning-en-aws-una-demo-completa/">Imagen</a></p>
					</section>
					
				</section>
				<section>
					<section data-transition="zoom" data-background="images/cbms_fondo03.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">2. Procesamiento Lenguaje Natural</h2>
					</section>
					<section>
						<h2>Definición PLN:</h2>
						<p>Es un campo dentro de la <strong>inteligencia artificial</strong> y <strong>la lingüística aplicada</strong> que estudia las interacciones
							mediante uso del lenguaje natural entre los seres <strong>humanos y las máquinas</strong>. 
							Más concretamente se centra en el <strong>procesamiento de las comunicaciones humanas</strong>, 
							dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. 
							Con la Comprensión y Generación de Lenguaje Natural, busca que las <strong>máquinas</strong> consigan entender, 
							interpretar y manipular el <strong>lenguaje humano</strong>.
						 </p>
						<p style="font-size: 15px">Fuente(decisiones.es): <a href="https://decidesoluciones.es/procesamiento-del-lenguaje-natural-pln-o-nlp-que-es-y-para-que-se-utiliza/">decisiones.es</a></p>
						
					</section>
					<section>
						<h3>Modelos para el PLN (1/2)</h3>
						<p>Tratar computacionalmente una lengua implica un proceso de <strong>modelización matemática</strong>. </p>
						<p><small>Existen dos aproximaciones generales al problema de la modelización lingüística:</small></p>
						<ol>
							<li>Modelos Lógicos: <strong>gramáticas</strong></li>
							<ul>
								<li><small>Los lingüistas escriben reglas de reconocimiento de patrones estructurales, 
								           empleando un formalismo gramatical concreto. </small> </li>
							     <li><small>Reflejan la estructura lógica del lenguaje y surgen a partir de las teorías de N. Chomsky en los años 50. </small></li>
							</ul>
							
						</ol>
						 
						<p style="font-size: 15px">Fuente: <a href="https://www.iic.uam.es/inteligencia/que-es-procesamiento-del-lenguaje-natural/">IIC</a></p>
					</section>
					<section>
						<h3>Modelos para el PLN (2/2)</h3>
						<ol>	
						<li>Modelos probabilísticos del lenguaje natural: <strong>basados en datos</strong></li>
							<ul>
								<li><small> 
									La idea es que basado en colecciones de ejemplos y datos <strong>(corpus)</strong> y a partir de ellos se calculan las <strong>frecuencias</strong> de diferentes unidades lingüísticas 
									(letras, palabras, oraciones) y su <strong>probabilidad</strong> de aparecer en un contexto determinado.
								</small></li>
								<li><small> 
								         Calculando esta probabilidad, se puede <strong>predecir</strong> cuál será la siguiente unidad en un contexto dado, sin necesidad de recurrir a reglas gramaticales explícitas.
								</small></li>
								<li><small>Es el <strong>paradigma de “aprendizaje automático”</strong> que se ha impuesto en las últimas décadas en <strong>Inteligencia Artificial</strong>: 
								        los algoritmos infieren las posibles respuestas a partir de los datos observados anteriormente en el corpus.
							 	</small></li>
								</ul>
							</ul>
						
						<p style="font-size: 15px">Fuente: <a href="https://www.iic.uam.es/inteligencia/que-es-procesamiento-del-lenguaje-natural/">IIC</a></p>
					</section>
					<section>
						<h3>Componentes del Procesado Natural</h3>
							
						<p>Los análisis se aplican dependiendo del objetivo de la aplicación:</p>
						<small> 
							<ol>
								<li>Análisis <strong>morfológico o léxico</strong>. Consiste en el análisis interno de las palabras que forman oraciones para extraer lemas, rasgos flexivos, unidades léxica compuestas. 
								Es esencial para la información básica: categoría sintáctica y significado léxico.</li>
								<li>Análisis <strong>sintáctico</strong>. Consiste en el análisis de la estructura de las oraciones de acuerdo con el modelo gramatical empleado (lógico o estadístico).</li>
								<li>Análisis <strong>semántico</strong>. Proporciona la interpretación de las oraciones, una vez eliminadas las ambigüedades morfosintácticas.</li>
								<li>Análisis <strong>pragmático</strong>. Incorpora el análisis del contexto de uso a la interpretación final. Aquí se incluye el tratamiento del 
								    lenguaje figurado (metáfora e ironía) como el conocimiento del mundo específico necesario para entender un texto especializado.</li>
							</ul>
						</small> 
						<p style="font-size: 15px">Fuente: <a href="https://www.iic.uam.es/inteligencia/que-es-procesamiento-del-lenguaje-natural/">IIC</a></p>
					</section>
					<section>
						<h3>Aplicaciones y usos Lenguaje Natural (1/2):</h3>
						<img src="images/PLN-Aplicaciones.png" style="float: left; border: 0; padding: 0px; width: 100%; height: 100%"></img>
						<p style="font-size: 15px">Fuente: <a href="https://www.iberdrola.com/innovacion/procesamiento-lenguaje-natural-pln">IBerdrola Innovación</a></p>
					</section>
					<section>
						<h3>Aplicaciones y usos Lenguaje Natural (2/2):</h3>
						<small>
						<ol>
							<li><strong>Resumen de textos</strong>, consiste en encontrar la idea principal del texto e ignorar lo que no sea relevante.</li>
							<li><strong>ChatBots</strong>, deberán ser capaces de mantener una charla fluida con el usuario y responder a sus preguntas de manera automática.</li>
							<li><strong>Generación automática</strong> de keywords y generación de textos.</li>
							<li><strong>Reconocimiento de entidades</strong>, encontrar personas, entidades comerciales o gubernamentales, países, ciudades, marcas…etc.</li>
							<li><strong>Análisis de sentimientos</strong>, deberá comprender si un tweet, una review o comentario es positivo o negativo y en qué magnitud (neutro). Muy utilizado en redes sociales, en política, opiniones de productos y en motores de recomendación.</li>
							<li><strong>Machine Translation</strong>, Ofrece la posibilidad traducir el texto o el audio de un idioma a otro rápidamente y cada vez con más exactitud.</li>
							<li><strong>Clasificación automática de textos</strong>,  en categorías pre-existentes, detectar temas recurrentes y crear las categorías.</li>
							
						</ol>
						</small>
						<p style="font-size: 15px">Fuente: <a href="https://www.baoss.es/procesamiento-del-lenguaje-natural-pln-con-python/">Baoss.es</a></p>
					</section>
					<section>
						<h3>Técnicas clásicas utilizadas en el análisis:</h3>
						<small>
						<ol>
							<li><strong>Tokenizar</strong>: separar palabras del texto en entidades llamadas tokens, con las que trabajaremos luego. Deberemos pensar si utilizaremos los signos de puntuación como token, 
								si daremos importancia o no a las mayúsculas y si unificamos palabras similares en un mismo token.</li>
							<li><strong>Tagging Part of Speech (PoS)</strong>: Clasificar las oraciones en verbo, sustantivo, adjetivo preposición, etc
							<img src="images/Tokenizar01.png" style="float: left; border: 0; padding: 0px; width: 100%; height: 100%"></img></li>
							
							<li><strong>Shallow parsing / Chunks</strong>: Sirve para entender la gramática en las oraciones.</li>
							<li><strong>Bag of words</strong>: (Sparse Vector)es una manera de representar el vocabulario que utilizaremos en nuestro modelo y consiste en crear una matriz en la que cada columna es un token y se contabilizará 
								la cantidad de veces que aparece ese token en cada oración (representadas en cada fila).</li>
							<li><strong>Word Embedding</strong>, representación de las palabras en un espacio vectorial, según el contexto..</li>
						</ol>
						</small>
						
						<p style="font-size: 15px">Fuente: <a href="https://www.baoss.es/procesamiento-del-lenguaje-natural-pln-con-python/">Baoss.es</a></p>
					</section>
				</section>
				<section>
					<section data-transition="zoom"; data-background="images/cbms_fondo01.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">3. Word embedding:</h2>
					</section>
					<section>
						<h2>La Idea inicial ya la dió Cervantes: </h2>
						<img src="images/DonQuijote.jpg" style="float: left; border: 0; padding: 0px; width: 40%; height: 40%"></img>
						En España tenemos un refran:
						<blockquote class="fragment fade-up" cite="https://cvc.cervantes.es/lengua/refranero/ficha.aspx?Par=58521&Lng=0">
							&ldquo;Dime con quien andas y te diré quien eres&rdquo;
						</blockquote>
						<small>[El Quijote II, 10 y 23]</small>
					</section>
					<section>

						<pre><code data-trim style="text-align: center;">El significado semántico de una palabra, 
depende de las palabras que las rodea.</code></pre>

						<p><strong>Word Embedding</strong> está basado en la idea de que la información contextual 
							por sí sola constituye una representación viable de términos lingüísticos</p>
						<p>In <em>en lingüistica computacional</em>, se usa el término de <strong>modelo de distribución semántica</strong>, 
							<em>las entidades lingüisticas con una distribución similar tiene mismo significado.</em> </p>
					</section>
					<section>
						<h3>Evolution:</h3>
						<ul>
							<li>In 2003 <strong>Bengio et al.</strong> propuso un modelo de lenguaje neuronal que aprendía de la depresentación en la distribución de palabras.</li>
							<li class="fragment fade-up"><strong>Collobert and Westorn</strong>(2008) fue el primer trabajo que mostró la utilizada de los moddelos pre-entrenados de "word embeddings"</li>
							<li class="fragment fade-up">In 2013 <strong>Mikolov et al.</strong> propuso <em>Word2Vec</em>, con dos enfoques, 
								<em>continous bag-of-word</em> and <em>skip-gram</em>, para construir una representación de distribución de vectores de gran calidad.</li>
						</ul>
					</section>
					<section>
						<h1>¿Qué es Word2Vec? </h1>
					</section>
					<section>
						<strong><em>Word2Vec</em></strong> es una red neuronal superficial con una sola capa oculta, donde se generan las palabras embebidas.
						<img src="images/W2V-Architecture.png" style="border: 0; padding: 0px; width: 60%; height: 60%"> </img>	

						<small>				
							<ol>
								<li>La entrada es<strong>un vector "One hot"</strong> con todas las palabras del texto</li>
								<li>una capa oculta, con <strong>size</strong> el tamaño del vector de cada palabra. </li>
								<li>Una capa de salida, utilizada como  <strong>SoftMax</strong> clasificador.</li>
							</ol>
						</small>
	
					</section>
					<section>
						<p>Word2vec tiene dos enfoques CBOW(Continuous bag of words) y  Skip-gram model.</p>
						<p><small>Aprende los pesos que actuan en cada representación del vector con un <strong>tamaño de ventana</strong>, sobre el contexto de cada palabra.</small></p>
						<img src="images/CBOW-SkipGram.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img>
						<p style="font-size: 20px">The training method of word2vec is backpropagation with stochastic gradient descent.</p>
						<img src="images/CBOW-ObjectiveFunc.png" style="float: left; border: 0; padding: 0px; width: 40%; height: 40%"> </img>
						<img src="images/SkipGram-ObjectiveFunc.png" style="float: right;border: 0; padding: 0px; width: 40%; height: 40%"> </img>
					</section>
					<section>
						<p><strong>El truco de Word2Vec es:</strong></p>

						<p>No tiene una función de activaciń en la capa oculta de las neuronas, utiliza como clasificador en la salida Softmax, 
							para la generación de la distribución de probabilidad.</p>
						
						<p>Realmente, la meta es aprender los pesos de la capa oculta, ya que esta será los vectores de cada palabra.</p>
						<blockquote>La capa oculta representa el vector de cada palabra dentro del espacio vectorial generado.</blockquote>
					</section>
					<section>
						<h2>Distancia de Similaridad:</h2>
						<p>Se utiliza la distancia del Coseno, para identificar la similaridad entre conceptos (vectores)</p>
						<img src="images/cosine-similarity.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img>
						<p>Este sistema es el que se utiliza para identificar a las palabras según el contexto.:</p>
					</section>
					<section>
						<h2>Caso de uso Snomed2Vec</h2>

						<p>Aplicación de "Word Embeddings", adaptando Word2Vec a Snomed-CT.</p>
						<p> Paper presentado en el Congreso internacional CBMS2019</P>
						<p style="font-size: 30px">Acceso a la presntación: <a href="https://nachuss.github.io/cbms2019/#/">Presntación del Articulo Snomed2Vec. Congreso CMBS2019 - Cordoba</a></p>
				
					</section>
				</section>
				


				<section>
					<section data-transition="zoom" data-background="images/cbms_fondo04.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">4. Transformers:</h2>
					</section>	
					<section>
						<h2>Transformers:</h2>
						<small>
						<p>En un paper de Google de finales del año 2017 <a href= "https://arxiv.org/abs/1706.03762" target="_blank">- “Attention is All You Need” - </a> se presentó la arquitectura del Transformer, 
							un modelo que tenía como principal innovación la sustitución de las <strong>capas recurrentes</strong>, como las LSTMs que se venían usando 
							hasta ese momento en PLN, por las denominadas <strong>capas de atención</strong>.</p>

						<p>Estas capas de <strong>atención</strong> codifican cada palabra de una frase en función del resto de la secuencia, 
							permitiendo así introducir el contexto en la representación matemática del texto, motivo por el cual a los 
							modelos basados en Transformer se les denomina también Embeddings Contextuales. </p>
						<p>Los Word Wmbeddings que se generan, son dinámicos, dependiendo del contexto.</p>
						</small>	
					</section>
					<section>
						<h2>Diseño(1/2):</h2>
						<img src="images/Transformes-EncDec.png" style="border: 0; padding: 0px; width: 45%; height: 45%"> </img>
						<p style="font-size: 15px">Fuente(iia.uam.es): <a href="https://www.iic.uam.es/innovacion/transformers-en-procesamiento-del-lenguaje-natural/">Instituto Conocimiento.es</a></p>
						
					</section>
					<section>
						<h2>Diseño(2/2):</h2>
						<img src="images/Transformes-QKV.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img>
						<!-- <img src="images/Transformes-MultiHead.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img> -->
					</section>
					<section>
						<h3>BERT:</h3>
						<p>Bidirectional Encoder Representations from Transformers</p>
						<img src="images/BERT01.png" style="border: 0; padding: 0px; width: 40%; height: 40%"> </img>
						<small>
						<p>BERT es un framework de propósito general en una gran variedad de treas de NLP, "Question Answering (SQuAD v1.1)", Clasificación (MNLI), y otros.</p>
						<p>El sistema aprende el lenguaje de una gran cantidad de texto plano, interpretando y comprendiendo la cohesión del texto. Permitiendo que se pueda afinar los ajustes, para adaptarlo a cualquier tarea específica.</p>
						</small>
					</section>
					<section>
						<h3>BERT aplicaciones principales:</h3>
						<p>Ajustes dependiendo del problema:</p>
						<img src="images/BERT02.png" style="border: 0; padding: 0px; width: 30%; height: 30%"> </img>
						<small>
						<ul>
						    	<li>Tareas de <strong>Clasificacion</strong>, como análisis de sentimientos.</li>
							<li>Preguntas y Respuetas <strong>(SQuAD)</strong>. Se le hace una pregunta y responde según el texto.</li>
							<li><strong>NER</strong>. (Reconocimiento de Entidades Nombradas). Se marca las plabras según el tipo de Entidad( Persona, Organizacion, Fecha...)</li>
							<li><strong>Resumen de Textos</strong>. Permite realizar un resumen del contenido de un texto, al extraer los conceptos principales. </p>
						</ul>
						<p>Todo estos procesos son posibles al ajuste en el entrenamiento del modelo (Fine-Tuning)</p>
						</small>
					</section>
					<section>
						<h2>Transfer Learning:</h2>
						<p><img src="images/TransferLearning.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img> </p>
						<p><img src="images/TransferLearning01.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img> </p>

					</section>
					<section>
						<h3>Comunidad HuggingFace:</h3>
						<p style="font-size: 15px">Fuente(iia.uam.es): <a href="https://huggingface.co/">Huggingface</a></p>
						<p><img src="images/Hugginface01.png" style="border: 0; padding: 0px; width: 25%; height: 25%"> </img> </p>
						<p><img src="images/Hugginface02.png" style="border: 0; padding: 0px; width: 75%; height: 75%"> </img> </p>						
					</section>

				</section>

				<section>
					<section data-transition="zoom" data-background="images/cbms_fondo05.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">5. Casos de Uso Enfoque Clinico:</h2>
					</section>	
					<section>
						<h2>Análisis HCE:</h2>
						<p><img src="images/CasoUso-HCE.png" style="border: 0; padding: 0px; width: 100%; height: 100%"> </img> </p>
					</section>
					
					<section>
						<h3>Mapeo Conceptos Clínicos:</h3>
						<small><p>Permite identificar el diagnóstico y procedimientos, desde el texto libre.
							asociando el concepto a un código de una terminología clínica.( ICD-10-MC,Snomed-CT..)</p></small>
						<img src="images/codification.png" style="border: 0; padding: 0px; width: 70%; height: 70%"> </img>
						<p>Normalemente este proceso es el que realiza un humano experto.</p>

					</section>
					<section>
						<h2>Enfoque de una herramienta NER:</h2>
						<p>Se propuso una herramienta para ayudar a los médicos a buscar en Snomed-CT y encontrar el código del concepto médico.</p>
						<p><strong><em>Snomed2Vec</em></strong>, es una herramienta de reconocimiento de entidades nombradas basada en ontologías que 
							utiliza "word embeddings", para sugerir cuál es el concepto más parecido de Snomed-CT, del que aparece en un texto.</p>
						<p><strong>Snomed2Vec</strong>, buscador semántico. Sugiere cual es el concepto mas similar, agrupado por sus jerarquías, de Snomed-CT.</p>
					</section>

				</section>
								
				<section>
					<section data-transition="zoom" data-background="images/cbms_fondo06.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">6. Librearias y Frameworks:</h2>
					</section>	
					<section>
						<h2>Librerias adaptadas a Español:</h2>
						<ul>
							<li><a href="http://nlp.lsi.upc.edu/freeling/index.php/node/1" target="_blank">Freeling, libreria de procesamiento de textos</a></li>
							<li><a href="https://spacy.io/" target="_blank">Spacy, libreria multi lenguaje para python</a></li>
							<li> <a href="https://nlp.johnsnowlabs.com/" target="_blank">John Snow Labs.</a></li>
						</ul>
					</section>
					<section>
						<h2>Spark-NLP:</h2>
						<p><img src="images/Spark-NLP02.png" style="border: 0; padding: 0px; width: 70%; height: 70%"> </img></p>
						<p><img src="images/Spark-NLP03.png" style="border: 0; padding: 0px; width: 70%; height: 70%"> </img></p>
					</section>
				</section>				
				<section>

					<section data-transition="zoom" data-background="images/cbms_fondo07.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">7. Referencias en España de PLN:</h2>
					</section>
					<section>
						<h3>Plan Nacional de Tecnologías del Lenguaje:</h3>
						
						<small><p>El objetivo general del Plan de Impulso de las Tecnologías del Lenguaje es desarrollar la industria 
							del procesamiento del lenguaje natural, la traducción automática y los sistemas conversacionales en España
							</p>
							<img src="images/PlanTecLeng.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img
							<a href="https://github.com/PlanTL-SANIDAD/lm-spanish" target="_blank">Repositorio Github, Modelos y Utilidades</a>
						</small>
					</section>
					<section>
						<h3>Unidad Tecnica Barcelona Super Computador:</h3>
						<img src="images/temu-bsc.png" style="border: 0; padding: 0px; width: 60%; height: 60%"> </img
						<a href="https://temu.bsc.es/" target="_blank">Unidad de Mineria de Textos</a>
					</section>

					<section>
						<h3>Plan de Tecnologías del Lenguaje:</h3>
						<p>Proyectos y Tareas <strong>(sharedTask)</strong>, para la mejora lenguaje Médico:</p>
						<img src="images/TemuCampanas.png" style="border: 0; padding: 0px; width: 50%; height: 50%"> </img
						<small>
						<ul>
							<li>MEDDOCAN:<a href="https://temu.bsc.es/meddocan/" target="_blank">Medical Document Anonymization Track</a></li> </li>
							<li>MEDDOPROF:<a href="https://temu.bsc.es/meddoprof/" target="_blank">MEDical DOcuments PROFessions recognition shared task</a></li>
							<li><a href="https://github.com/PlanTL-SANIDAD/lm-spanish" target="_blank">Repositorio Github, Modelos y Utilidades</a></li>
						</ul>
						</small>
					</section>

				</section>					
				<section>				
					<section data-transition="zoom" data-background="images/cbms_fondo08.jpg">
						<h2 style="display:block;background:rgba(235,235,235,0.7);color:#404040;padding:13px 10px 10px 10px;border-radius:3px;">8. Conclusiones y Propuestas:</h2>
					</section>
					<section>
					<h2>Conclusiones:</h2>
					<ol>
						<li>Es posible utilizar las últimas técnicas de PLN al ámbito médico ocn resultados prometedores</li>
						<li>Seria interesante poner en común que se está haciendo en otras Comunidades sobre este tema.</li>
						<li>Repositorio de Soluciones PLN en Sanidad</li>


					</ol>
					</section>				
				</section>
				<section style="text-align: left;">
					<h1>THE END</h1>
					<p>
						Autores: <br>
						<img src="images/QR-PLNClinical.png" style="float: right; border: 0; padding: 0px; width: 30%; height: 30%"></img>
						- <a href="https://nachuss.github.io/" target="_blank">Ignacio Martínez Soriano</a> <br>

					</p>
					<hr>
					<p>
					<!-- <a target="_blank" href="https://docs.google.com/forms/d/e/1FAIpQLSdGoKAMT_jykJTzb2SnHiSXK-rTqSzm2HLG4Q0DdIzR0xbrhQ/viewform">Link to: Please fill Satisfaction Survey</a> -->
					
				</p>
				</section>

				

			</div>

		</div>

	<div style="position: absolute; bottom: 10px; left: 100%;  margin-left: -150px">
				<img src="images/logoSnomed.png">
	</div>			
		<script src="js/reveal.js"></script>
		<script src="lib/highlight.js"></script>

		<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				controlsLayout: 'edges',
				progress: true,
				slideNumber: true,
				center: true,
				hash: true,
				rollingLinks: true,
				transition: 'convex', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'plugin/search/search.js', async: true },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
